{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from surprise import SVD, Reader, Dataset, accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# settings\n",
    "from IPython.core.display import HTML\n",
    "pd.set_option('display.max_rows', 2000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# data viz imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# load data\n",
    "df1_phaseIII = pd.read_csv('Data/phaseII_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>userId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2000-07-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1996-11-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2005-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>15</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2017-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>17</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2011-05-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>18</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2016-02-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>19</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2000-08-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>21</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2014-08-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>27</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2000-07-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>31</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1996-12-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId             title                                       genres  userId  rating   timestamp\n",
       "0        1  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy       1     4.0  2000-07-30\n",
       "1        1  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy       5     4.0  1996-11-08\n",
       "2        1  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy       7     4.5  2005-01-25\n",
       "3        1  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy      15     2.5  2017-11-13\n",
       "4        1  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy      17     4.5  2011-05-18\n",
       "5        1  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy      18     3.5  2016-02-11\n",
       "6        1  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy      19     4.0  2000-08-08\n",
       "7        1  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy      21     3.5  2014-08-09\n",
       "8        1  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy      27     3.0  2000-07-04\n",
       "9        1  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy      31     5.0  1996-12-13"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# glimpse at top 10 records\n",
    "df1_phaseIII.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df1_phaseIII[['userId', 'movieId', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8724509666249268"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# We'll use the famous SVD algorithm.\n",
    "algo = SVD()\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(train)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training and testing our algorithm for our data, we see that the accuracy of the algorithm is around 87.8%. This result inidcates that our model's prediction ability is good.\n",
    "\n",
    "We can also consider a few more factors and methods to evaluate the model and improve the performance of our model.\n",
    "    \n",
    "__Cross Validation__\n",
    "\n",
    "Cross Validation is performed in order to reduce the bias that may have happened when splitting the data between test set and training set. Cross validation divided the data into a specified number of sets, n (usually a default of 5 sets) and performs training on n-1 sets and uses 1 set as the test data to evaluate the performace of the model. This step is repeated till all the sets are used as test and train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.8765  0.8679  0.8842  0.8745  0.8639  0.8734  0.0070  \n",
      "MAE (testset)     0.6730  0.6664  0.6779  0.6723  0.6656  0.6710  0.0046  \n",
      "Fit time          5.10    4.43    4.92    5.03    4.95    4.89    0.23    \n",
      "Test time         0.12    0.24    0.12    0.20    0.14    0.16    0.05    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([0.8764662 , 0.86794284, 0.88421791, 0.87451768, 0.86389767]),\n",
       " 'test_mae': array([0.67295638, 0.66644103, 0.67794137, 0.67231241, 0.66556339]),\n",
       " 'fit_time': (5.097747564315796,\n",
       "  4.434219121932983,\n",
       "  4.924458980560303,\n",
       "  5.033463716506958,\n",
       "  4.948090553283691),\n",
       " 'test_time': (0.12167644500732422,\n",
       "  0.24139666557312012,\n",
       "  0.1186826229095459,\n",
       "  0.20081210136413574,\n",
       "  0.14165520668029785)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise.model_selection import cross_validate\n",
    "cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<H3>Findings </H3>\n",
    "\n",
    "According to the results seen above, RMSE score is around 87% which is very close to the results obtained during the model training and testing without cross validation.\n",
    "\n",
    "GridSearchCV\n",
    "Grid search is the process of performing hyper parameter tuning in order to determine the optimal values for a given model. This is significant as the performance of the entire model is based on the hyper parameter values specified.\n",
    "\n",
    "The hyperparamters here are\n",
    "\n",
    "n_epochs – The number of iteration of the Stochastic Gradient Descent(SGD) procedure. Default is 20\n",
    "lr_all - The learning rate for all parameters. Default is 0.005\n",
    "reg_all - The regularization term for all parameters. Default is 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import GridSearchCV\n",
    "param_grid = {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005],\n",
    "              'reg_all': [0.4, 0.6]}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)\n",
    "\n",
    "gs.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8935655773949193\n",
      "{'n_epochs': 10, 'lr_all': 0.005, 'reg_all': 0.4}\n"
     ]
    }
   ],
   "source": [
    "print(gs.best_score['rmse'])\n",
    "\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the above results, the optimal values for our hyperparameters are:\n",
    "\n",
    "n_epochs = 10\n",
    "lr_all = 0.005\n",
    "reg_all = 0.4\n",
    "Using these values in our SVD algorithm, we need to check for the accuracy rate of our model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8862073935894843"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "algo1 = SVD(n_epochs= 10, lr_all= 0.005, reg_all =  0.4)\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo1.fit(train)\n",
    "predictions = algo1.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Inference:__\n",
    "As you can see from the above results, after finding the optimal paramters for the model using GridSearchCV, the performace of the model improved 600 basis points, which is very good.\n",
    "\n",
    "Model Accuracy Analysis\n",
    "Prediction accuracy answers the question how well the recommender does at estimating preference\n",
    "\n",
    "Decision support metrics answers how well the recommender does at finding good things\n",
    "\n",
    "Rank accuracy metrics look at how well the recommender estimates relative preference\n",
    "\n",
    "Metrics Families:\n",
    "1. Fraction of Concordant Pair\n",
    "2. Mean Absolute Error\n",
    "3. Mean Squared Error\n",
    "4. Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Fraction of Concordant Pair__\n",
    "\n",
    "Looks at the fraction of all pairs that it puts in the correct order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCP:  0.6702\n",
      "0.6701777245366562\n"
     ]
    }
   ],
   "source": [
    "from surprise.accuracy import fcp\n",
    "print(fcp(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence according to this metrics it puts around 67.02% of the pairs in the correct order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mean Absolute Error__\n",
    "MAE=1|𝑅̂ * ∑ r𝑢𝑖∈𝑅̂ |𝑟𝑢𝑖−𝑟̂𝑢𝑖|\n",
    "\n",
    "This gives us the abosulte mean error of predicted values and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.7854\n",
      "0.7853635444526672\n"
     ]
    }
   ],
   "source": [
    "from surprise.accuracy import mse\n",
    "print(mse(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence according to this metrics, its prediction accuracy is around 78.53%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Root Mean Squared Error__\n",
    "RMSE=(1|𝑅̂ * ∑ r𝑢𝑖∈𝑅̂ |𝑟𝑢𝑖^2−𝑟̂𝑢𝑖^2|) ^ 0.5\n",
    "\n",
    "If you observe the formula, this is the square root of the Mean sqaured error computed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8862\n",
      "0.8862073935894843\n"
     ]
    }
   ],
   "source": [
    "from surprise.accuracy import rmse\n",
    "print(rmse(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is indeed the square root of .8862. The accuracy of this model is around 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Conclusion__\n",
    "\n",
    "We build a Collaborative Filtering based Reccommender System which has a very good accuracy of around 90%. Which will help the business retain customers and increase customer engagement in the entertainment platform.\n",
    "\n",
    "There is definitely scope for improvement by taking more information about users and movies.\n",
    "\n",
    "__Recommendation__\n",
    "\n",
    "When the user uses the imdb database for the first time, it is recommended that the system asks the user to rate a set of movies and this helps to prevent the cold start problem, wherein, we do not have enough information about the user preferences and hence recommendation becomes very difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25209"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
